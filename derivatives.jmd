``` {r echo=FALSE, results="asis"}
## process with knitr:::knit2html("filename.Rmd")
## Nothing to see here ... move along
require("questionr", quietly=TRUE)
page <- questionr:::Page$new()
nav <- questionr:::NavBar$new()
cat(page$write_header())
```

`r I(nav$write_header("Approximate derivatives in julia", "using finite differences"))`



`r I(nav$add("Slope", "of tangent line"))`


The derivative of a function, $f$, at a point $c$ is the slope of the
_tangent line_ and is defined in terms of the limit of the slopes of
approximating secant lines. The notation is:

$$
f'( c ) = \lim_{h \rightarrow 0} \frac{f(c + h) - f( c)}{h}
$$

Intuitively, the tangent line is the best straight line approximation
to a function near the point $(c, f( c))$.

This graph shows $f(x) = 2 - x^2$ at various secant lines at $c=-0.75$:

```j
f(x) = 2 - x^2; c = -0.75
sec_line(h) = x -> f(c) + (f(c + h) - f(c))/h * (x - c)
plot([f, sec_line(1), sec_line(.75), sec_line(.5), sec_line(.25)], -1, 1) | orender
```

As the value of $h$ goes towards 0 along the path 1, .75, .5, .25,
... the slope of the secant line heads towards the slope of the
tangent line, in this case $2\cdot 0.75 = 1.5$.

Using the idea of a derivative at a point, one defines the derivative
of the function $f$ to be the function which for each $x$ returns the
derivative of $f$ at the point $x$. Notationally, this just replaces
$c$ above with $x$, but conceptually there is a bit more to it.

The rules of derivatives allow for a mechanical approach to taking
derivatives with the power rule, chain rule and some special cases. In
this project we look at approaching the derivative numerically. This
is done using _approximate derivatives_ computed with finite differences.

The most basic approximation is simply to assume $h$ is some small
number and use that to approximate the limit above:

```j
f(x) = x^2 - 2x; fp(x) = 2x - 2
h = .001
c = 3
( f(c + h) - f( c) ) / h
```

This is known as the _forward difference_ approximation to the derivative. For this example, the difference between the approximation and the actual slope is:

```j			
( f(c + h) - f( c) ) / h - fp(c)
```

Notationally, we may write this as:

$$
f'( c) \approx ( f(c + h) - f( c) ) /h
$$

For some small value of $h$. The equals sign is replaced by an
approximation symbol, as there is no limit expression written.

<span class="label label-warning">Example: Derivative at a point</span>

Let's try the derivative of some well known function. We know the
derivative of $f(x) = \sin(x)$ is $f'(x) = \cos(x)$. Let's see if the above
works well:

```j
f(x) = sin(x); fp(x) = cos(x)
c = pi/4; h = 0.0001
( f(c + h) - f(c) )/h - fp(c)
```

Not as good as we can get, but not too bad already.

<span class="label label-warning">Example: Finding profit</span>

Suppose John Deere models its profit per units sold through the
function $P(x) = 50 e^{x/250}/(1 - e^{x/250})$. Find the marginal
profit for $x=200$. 

The marginal profit is the change in profit per unit sold -- or the
derivative -- at $x=200$.  We can find this quite simply by either
differentiating directly, or as we do here, approximating the answer
with `julia`. We first note that the function above, is a composition, so
write using two functions:

```j
f1(x) = exp(x)/(1 - exp(x))
f(x) = 50 * f1(x/250)
c = 200; h = 0.0001;
res = (f(c+h) - f(c))/h
```

If $50$ is the maximum profit, this is a percentage increase of:

```j
(res/50) * 100
```

or a bit more than half a percent.

<span class="label label-warning">Example: Finding the tangent line at a point</span>

Let $f(x) = x^x$. Find the tangent line at $c=2$. Compare the
difference between this and the function value at $x=2.1$.

The tangent line is most easily expressed in terms of the _point-slope_ formula for a line where the point is $(c, f(c))$ and the slope is $f'(c)$. This gives:

$$
y = f(c) + f'(c)\cdot(x - c)
$$

In the following, the slope of the tangent line will be _approximated_
using a numeric derivative.  We use $h=0.0001$:

```j
f(x) = x^x
c = 2; h = 0.0001
m = ( f(c + h) - f(c) ) / h	 
tangent_line(x) = f(c) + m*(x - c)
```

To compare the difference, we have:

```j
f(2.1) - tangent_line(2.1)
```


A graph shows this difference:

```j
plot([f, tangent_line], 1.95, 2.15) | orender
```


### questions

* repeat at 2.05
* find tangent line at ...
* what is intercept ( x = 0)

<hr/>


`r I(nav$add("Derivative", "of a function"))`


We might be interested not in the derivative at a point, but in the
derivative as a function. This is how we think of the derivative when
we say if $f(x) = \sin(x)$, the $f'(x) = \cos(x)$.  Mathematically,
from this viewpoint, the derivative is referred to as an _operator_, as it
takes a function and returns a function.

We can reproduce this behavior easily enough with `julia`, as
functions can be passed as arguments and returned as values. 

First, let's define a function to find the derivative at a point using
the "forward difference":

```j
forward_difference(f, x0, h) = (f(x0 + h) - f(x0))/h
```

We need three arguments of course as we have three ingredients to
worry about: the function, the point and the size of $h$.

To make an operator that takes $f$ and returns a function (an
anonymous function in this case) that computes our approximation to
$f'$ we can do the following:

```j
D(f, h) = x -> forward_difference(f, x, h)
D(f) = D(f, 1e-8)
```

The first form allows us to specify $h$, the second chooses some fixed
value of $h$ for convenience.


To see this work, we can find and plot the derivative of $\sin(x)$:

```j
f(x) = sin(x)
fp(x) = D(f)(x)
plot([f, fp], 0, 2*pi) | orender
```

Well, we already knew that should just be $\cos(x)$.  The point is we
can easily make the an approximate derivative function from `f` with
the definition `fp(x) = D(f)(x)`. The key here is first `D(f)` is
computed, returning a function which is then evaluated a specified
`x`.


Let's look at a different function, where we don't know in our heads the answer.

```j
f(x) = exp(x)/(1 + exp(x))
fp(x) = D(f)(x)
plot(fp, 0, 5) | orender
```

<span class="label label-warning">Example: Critical points</span>

A function's critical points are where its derivative is $0$ or
undefined. We can graphically find these by graphing the functions
derivative. Let's do so for the polynomial $f(x) = x^3 - 5x + 4$:

```j
f(x) = x^3 -5x + 4
fp(x) = D(f)(x)
plot(fp, -5, 5) | orender
```

You can check the zeroes graphically, say by zooming in a bit and adding the line $y=0$:

```j
plot([fp, x->0], -2, 2) | orender
```

If you want, you can take the derivative by hand $f'(x) = 3x^2 - 5$
and use the `roots` function to compare.

```j
load("poly")
p = [3, 0, -5]
roots(Polynomial(p))
```


<span class="label label-warning">Example: When is a function increasing?</span>

Let $f(x) = \cos(x)$ over the interval $[0, 2\pi]$. When is the
function increasing? Well, we know the answer is when $f'(x) > 0$. We
can find this directly as $f'(x) = -\sin(x)$ and we know when that is
positive (well you are supposed to anyways). Let's see how we would do
this with `julia` and compare.

First, we only need the derivative, so we just ask:

```j
f(x) = cos(x)
fp(x) = D(f)(x) ## use default h
```


To see where we are positive in the interval $[0, 2\pi]$ we first
define a discrete approximation to the interval:

```j
x = linspace(0, 2*pi, 1000)
```

Now we test whether $f'(x) > 0$ for each of these values. A convenient
way to do this is the `filter` function. If we write a function which
returns `true` or `false` for a given `x`, then `filter` will return
just those values matching `true`. Functions such as `filter` or `map`
are often used with _anonymous functions_ as below:

```j
x_positive = filter(x -> fp(x) > 0, x)
```

This idea is behind our `show_positive` function, which can be used to show where a function is positive:

```j
p = curve(f, 0, 2pi)
show_positive(p, fp, "blue") | render
```

### Question

`r I(page$new_problem("Show negative?"))`
`r I(page$new_problem("..."))`
`r I(page$new_problem("..."))`


<hr />

<i class="icon-eye-open"></i> <span class="label label-important">Higher-order functions</span>

There are a handful of functions like `filter` that are implemented in
many different programming languages (e.g. `map` and `reduce`). These
are termed _higher-order functions_ and have in common that they take
a function as an argument.

`r I(nav$add("Improvements", "to the basic forward difference equation"))`

The error in the approximation of the derivative depends on the size
of $h$. Mathematically, as $h$ gets smaller we get better
approximations, though with the computer other complications arise. To
see mathematically that this is the case, let's look at the difference
between the approximate numeric derivative from the forward difference
and a known derivative.


```j
f(x) = sin(x)
fp(x) = cos(x)
[ D(f,h)(.5) - fp(.5) for h=[.1, .01, .001, .0001, .00001] ] 
```

It gets better as $h$ gets smaller. Let's look a little deeper
though. Rather than type in the values of $h$ as above, let's use an
expression to compute them. Recall the "dot" notation allows us to do
powers with more than one value at once. Here we find the powers
$10^{-1}, \dots, 10^{-16}$ at once and then compute the differences:

```j
n = 1:16
out = [ D(f,h)(.5) - fp(.5) for h=(1/10).^(1:16) ] 
[n float(out)]
```

When we look, we see that for awhile the approximation gets better (to about `1e-9`),
but then does a U-turn and starts getting worse. The mathematical
approximation gets better and better, but what happens is the
compuational error gets worse. We'll see below how to pick the $h$
that best balances these off, but first lets look at how using
different approximations for the derivative can improve the
"mathematical" error.

We like this tablular display, but it is a bit cumbersome to type. As
such we make a simple function to automate the task.

```j
table(f, n) = [1:n float([f(x) for x in (1/10).^(1:n)]) ]
table(h -> D(f,h)(.5) - fp(.5), 5)
```


`r I(nav$add("Central difference"))`

It turns out that just by looking to the left and the right we can
improve the mathematical error from getting better at a rate of $h$ to
a rate of $h^2$, provided our function is smooth enough. The formula,
called the _central difference_ approximation to the derivative is:

$$
f'(x) \approx \frac{ f(x + h) - f(x - h) }{2h}
$$

This has the same order of numeric error, but the mathematical error is like $h^2$, not $h$.


Let's compare. To make our life easier we again create some functions, as we did with `D` above.

```j
central_difference(f, x, h) = (f(x+h) - f(x-h))/(2h)
Dc(f, h) = x -> central_difference(f, x, h)
Dc(f) = Dc(f, 0.0001)
```


<i class="icon-eye-open"></i> <span class="label label-important">Qualifying types</span>

It is better in `julia` to qualify the argument types. For example
```j
central_difference(f::Function, x::Real, h::Real) =  (f(x+h) - f(x-h))/(2h)
Dc(f::Function, h::Real) x -> central_difference(f, x, h)
Dc(f::Function) = Dc(f, .0001)
```

The reason this is better, as `julia`'s multiple dispatch is used to
sort out which rule to apply and specifying more allows more user
convenience. 

<hr/>

Now to see which works a little better. We have again with $f(x) = \sin(x)$

```j
f(x) = sin(x)
fp(x) = cos(x)
[ D(f,h)(.5) - fp(.5) for h=[.1, .01, .001, .0001, .00001] ] 
[ Dc(f,h)(.5) - fp(.5) for h=[.1, .01, .001, .0001, .00001] ] 
```

The errors for the central difference are much smaller for the same
size $h$ (for the values used above). The central difference is more
efficient. This is a good thing -- a little mathematics can save some CPU time.


<span class="label label-warning">Example: When does tangent line hit 0?</span>

Let $f(x) = 10/(1+x^2) - 10\exp(-(1/2)x^2)$. The tangent line at $x=c$ is given by

$y = f( c) - f'( c)(x - c)$ and this intersects the $x$ axis when $y=0$. Solving this gives:
$$
c - f( c)/f'( c)
$$

Our goal is to compute this value for any $c > 0$.

Doing so is easy:

```j
f(x) = 10/(1+x^2) - 10*exp(-(1/2)*x^2)
fp(x) = Dc(f)(x)
intersection_point(c) = c - f(c)/fp(c)
```

For example, when $c=1$ we have:

```j
c = 1; intersection_point(c)
```

You can tell from the graph of $f(x)$ that this value should be more than 1, as it is.

```j
plot([f, x -> 0, x -> f(c) + fp(c)*(x-c)], .5, 2.1) | orender
```


### Questions
`r I(page$new_problem("Find the  ..."))`
`r I(page$new_problem("Find the  ..."))`
`r I(page$new_problem("Find the  ..."))`
`r I(page$new_problem("Find the  ..."))`

<hr/>


`r I(nav$add("Error analysis"))`


The choice of $h$ above was done somewhat arbitrarily, with only the
intuition that smaller $h$'s should produce more accurate
approximations (which of course may be way off, as we are subtracting
like-sized values in the derivative formula). Here we look a bit more
closely at how to choose $h$.


As mentioned, with a bit of work from the second semester of calculus
one can learn that the _mathematical error_ in the forward difference
is "order $h$" whereas the mathematical error in the central
difference is "order $h^2$. This means that as $h$ gets small the
approximation error in the first is a multple of $h$, but for the
second a multiple of $h^2$ -- a much smaller error in general.

However there is also error due to _floating point approximation_. Such
error is different in that it gets bigger as $h$ gets smaller. So one
error gets bigger, the other gets smaller. So clearly if $h$ gets too
small, the floating point error will dominate and the overall error
will not get smaller, rather larger.

So, how big is the floating point error? For any given number, it can
be roughly as big as the the machine precision amount, $\epsilon$. So in the
forward-difference approximation we could have errors in both
terms $f(x+h)$ and $f(x)$ so the total error could be as big as
$2\epsilon$. But this is divided by $h$ as we have:

$$
\frac{f(x+h) - f(x)}{h} = \frac{float(x +h) + \epsilon - float(x) + \epsilon}{h} = 
	     \frac{float(x+h)-float(x)}{h} + \frac{2\epsilon}{h}.
$$

The errors may or _may not_ cancel so the algebra with $\epsilon$ is unusual to
the untrained eye. It basically takes into account the worse case.

The key is the $2\epsilon/h$ term -- it gets bigger as $h$ gets smaller.

So if each floating point approximation is no more off than
$\epsilon$, we have this bound on the error:

$$
\text{error} \leq \frac{2\epsilon}{h} + (M/2)h
$$

Where $M$ is a constant that depends on the maximum absolute value of the
second derivative and the $1/2$ comes from second semester
calculus. Choosing $h$ to make this as small as possible is possible
with some calculus involving the derivative, but for now we simply
note the answer is $h=((2\epsilon)/(M/2))^{1/2}$.

Taking $\epsilon$ as machine tolerance and (conveniently) $M=1$ we get

$$
h_{min} = \sqrt((2\epsilon)/(1/2)) \approx 10^{-8}
$$

Or more precisely:

```j
sqrt( (2*eps())/(1/2) )
```


We can check how this works for our example from above:

```j
table( h -> D(f,h)(.5) - fp(.5), 16)
```

We see the best case of these is for $h=10^{-8}$, as expected by the theory.

`r I(nav$add("Approximation errors", "For the central difference"))`

For the central difference, the errors are different. The error in $h$ becomes:

$$
\text{error} \leq (2\epsilon)/(2h) + (M/6) h^2
$$


This gives $(3\epsilon/M)^{1/3}$ as the choice for $h$ that minimizes the right-hand expression.

Taking $\epsilon$ as the machine tolerance and (conveniently) $M=3$ we get

$$
h_{min} \approx 10^{-6}
$$

We can again check how this works for our function and point:

```j
table(h -> Dc(f,h)(.5) - fp(.5), 12)
```

<span class="label label-warning">Example: "Best" depends on the function and value</span>

A subtle point above is that we are minimizing an upper bound in $h$
with an assumption, not the actual error. The actual answer may differ
as it depends on a function and the point of evaluation. Look at the
function $f(x) = x^2 - 2x$ and compare the values at $0.5$ as
above. What is the best value of $h$?

```j
f(x) = x^2 - 2x
fp(x) = 2x - 2
c = 0.5; n = 1:12
table(h -> Dc(f,h)(c) - fp(c), 12)
```





`r I(nav$add("Better approximations"))`


One can create successively better mathematical approximations if more
steps are used. (E.g., see
http://en.wikipedia.org/wiki/Finite_difference_coefficients .) This
next approximiate derivative uses 4 nearby steps:


$$
f'(x) \approx \frac{-f(x + 2h) + 8f(x + h) - 8f(x-h) + f(x - 2h)}{12h}
$$
and has order $h^4$.

```j
central_4th_difference(f, x, h) = (-f(x + 2h) + 8f(x+h) - 8f(x-h) + f(x-2h))/(12h)
Dc4(f, h) = x-> central_4th_difference(f, x, h)
Dc4(f) = Dc4(f, 0.001)
```

Since this is the best we are going to do, we define the usual `'` notation through the following trick:

```j
import Base.ctranspose 
ctranspose(f::Function) = Dc4(f)
```

Then we have:

```j
f(x) = x^2
f'(1) ## should be 2(1) = 2
```


Under assumptions, the error with this approximation is bounded above with:

$$
\text{error} \leq (18/12)\epsilon/h + (M/30) h^4
$$

This has minimum value

$$
h_{min} = 2^{-2/5} (30\cdot 18	\epsilon/(12M))^{1/5}
$$

If we take $M=1$ then this is approximately $10^{-3}$.

We can again check:

```j
f(x) = sin(x)
fp(x) = cos(x)
table( h->  Dc4(f,h)(.5) - fp(.5), 8)
```


#### Questions

`r I(page$new_problem("Using ' notation"))`
`r I(page$new_problem("Using ' notation"))`
`r I(page$new_problem("Using ' notation"))`
<hr />



`r I(nav$add("Automatic differentiation"))`

If you load the helper functions with the command:

```j
using Gadfly, Compose, TeachersHelpers; load_gist("4530920");
```

Then you will also load in some code to do _automatic differentiation_
(http://en.wikipedia.org/wiki/Automatic_differentiation). Automatic
differentiation avoids the numeric instability issues of finite
differences by using a different approach.

Consider the function $f(x) = \cos{x^2 - x} = h(g(x))$ and its
derivative $f'(x) = h'(g(x)) g'(x) = \sin(x^2 - x) \cdot (2x - 1)$. To
compute this at 3, say, and working from right to left, we have to
compute $g'(3) = (2(3) - 1)$ then $g(3) = 3^2 - 3)$ and finally
$f'(g(3)) = \sin(3^2 - 3)$. The idea of automatic differentiation is
while the computer is composing like this:

```
x -> g(x) -> h(g(x))
```

it actually stores

```
(x,x') -> (g(x), g'(x) x') -> (h(g(x)), h'(g(x)) g'(x) x')
```

It looks more complicated, but in each step we just have `(u, u') ->
(g(u), g'(u) u')` as the chain rule would have us do. Then while the
function is computing itself in the first position, the derivative is
doing the same in the second position. So there is no loss due to
finite differences at the added cost of having to do two things
instead of 1.

This idea is implemented for the first and second derivative. To see,
say we have $f(x) = \cos(x^2)$ and we want to compute $f'(5)$.

To do so, we first liftup $x$ to carry the three values $(x, x', x'')
= (x, 1, 0)$ at $x=5$. This is done with the `ad` constructor:

```j
x = ad(5)
```

Let's ignore the third term and focus just on the first two.  (It
isn't much different, just messier.) Now our chain of operations is
read from inner to outer so we need to first square $x$ then take the
cosine.

If we look at $u -> u^2$ we have (from the chain rule) $(u^2, 2u \cdot
u')$. From above, we use $u = (x=5, x'=1)$ giving $(5^2,
2(5)(1))$. Let's see. Look at the first two terms of this:

```j
x^2
```

Then for $u -> cos(u)$, we should get $(\cos(u), -\sin(u) \cdot
u')$. With $u = (x=25, x'=10)$, this should be $(cos(25),
-sin(25)*10)$ = $(0.9912028118634736,1.3235175009777302)$. And indeed
we have:


```j
cos(x^2)
```

The second number is the derivative of the function at $5$ and is
computed along the way as accurately as the original value.  What
makes this interesting is there are no approximations, all done by
shadowing the usual operations. Further:

* compared to computing derivatives symbolically (such as can be done
  via the Wolfram alpha website) the procedure is as fast as computing
  the original expression,

* compared to numeric derivatives through finite differences, there is
  no round off error.

So when this method works, it is a good one to use. We've defined
operators `D` and `D2` to compute the first and second automatic
derivatives for most basic functions. This makes it easy to use when
applicable.

For example,

```j
f(x) = cos(x^2)
D(f)(5)
D2(f)(5)
```

Or we can plot

```j
f(x) = x^3 - 2x + 1
plot([f, D(f), D2(f)], -2, 3) | orender
```

How much more accurate are these derivatives? Let's compare for a
given function, say $f(x) = x^2(1-x)^{10}$.

```j
f(x) = x^2 * (1-x)^(10)
fp(x) = 2x*(1-x)^(10) - x^2*10*(1-x)^9
vstack(render(plot([x -> fp(x) - D(f)(x)], 0, 1)),    
       render(plot([x -> fp(x) - f'(x)], 0, 1)))   | orender
```

From the scale of the $y$ axes in the graph, you can see that the
automatic derivative is much close to the real derivative. Though such
differences won't effect our analysis in these projects, it may in
other applications.


The automatic derivative -- as implemented here -- is limited. For
example, you can't find `D2(f)` simply by applying `D` twice, as in
`D(D(f))`. So some care must be taken.


<!--- Finish this off -->
`r I(nav$write_footer())`
`r I(page$write_footer())`


