``` {r echo=FALSE, results="asis"}
## process with knitr:::knit2html("filename.Rmd")
## Nothing to see here ... move along
require("questionr", quietly=TRUE)
page <- questionr:::Page$new()
nav <- questionr:::NavBar$new()
cat(page$write_header())
```
`r I(nav$write_header("Newton's method", "using julia"))`

```{r echo=FALSE}
## helper function for plotting
plot_nm <- function(f, fp, x0, from, to, diff_tol=NA, max_ctr=25) {

  tol = 1/1000

  xs <- c(x0); ys <- c(0)
  ctr <- 0
  x = x0
  
  while(ctr < max_ctr && abs(f(x)) > tol) {
    xs <- c(xs, x); ys <- c(ys, f(x))

    x = x - f(x)/fp(x)

    xs <- c(xs, x); ys <- c(ys, 0)

    ctr = ctr + 1
  }

  curve(f, from, to, bty="L")
  lines(xs, ys, lty=2, col="gray")
  abline(h=0)

  points(xs[1], 0, pch=17, cex=1.4)
  points(x, 0, pch=16, cex=1.4)
  
  if(!is.na(diff_tol)) {
    xs = xs[which(ys == 0)]
    ind = c(1, 1 + which(abs(diff(xs)) > diff_tol))
    
    for(i in ind)
      text(xs[i], 0, sprintf("x[%s]", i-1), pos=1)
  }
  
}
```



Newton's method is an old method for _approximating_ a zero of a
function. Previously we discussed the _bisection method_ to solve for an $x$ where

$$
f(x) = 0
$$

for some continuous function $f(x)$. Newton's method puts additional
assumption on the differentiability of the function $f$, but is used
for the same problem of finding zeroes.


Unlike the bisection method which is slow but guaranteed to find a
root (the intermediate value theorem), Newton's method is fast (once
it is close) but has no guarantee. We'll see how to implement it, try
some examples, and then look at what can go wrong.

`r I(nav$add("Basic idea"))`

The idea behind Newton's method is simple: A function near a point is
well approximated by it's tangent line. If we have a good guess $x_n$, then
we can improve this guess iteratively by replacing it with the zero, $x_{n+1}$, of
the tangent line.


```{r echo=FALSE}
f <- function(x) log(x) - .5
plot.new()
xlim=c(.75, exp(.75))
plot.window(xlim=xlim, ylim=f(xlim))
x <- seq(xlim[1], xlim[2], length=1000)
lines(x, f(x))
abline(h=0, col="gray")

x0 = exp(.5) - .45
g = function(x) 1/x
tangent = function(x) f(x0) + g(x0)*(x-x0)
curve(tangent, add=TRUE, col="gray70")

x1 = x0 - f(x0)/g(x0)
text(x1, 0, expression(x[n+1]), pos=3)
text(x0, 0, expression(x[n]), pos=3)
text(exp(.5), 0, "zero", pos=1, offset=1)

text(xlim[1], f(x0), expression(f(x[n])))

lines(c(x0, x0), c(0, f(x0)), lty=2, col="gray70")
lines(c(xlim[1] + .075, x0), c(f(x0), f(x0)), lty=2, col="gray70")
```


A simple picture shows that we have a triangle with base $x_{n} -
x_{n+1}$, rise $f(x_n) - 0$ and slope $f'(x_n)$, using the "rise over
run" formula:

$$
f'(x_n) = \frac{f(x_n)}{x_{n} - x_{n+1}}.
$$

The basic algorithm of Newton's methods is then:

$$
x_{n+1} = x_n - f(x_n)/f'(x_n),
$$

which some books write  as $x_n - f'(x_n)^{-1} f(x_n)$, as this generalizes to higher dimensions.


The method is an _iterative method_. One begins with a (suitable)
guess $x_0$. From that the algorithm produces $x_1$ which is used to
produce $x_2$, etc. The idea is that one eventually will settle on a
value $x_n$ sufficiently close to the desired root.



Mathematically, the inidices indicate that the right hand side is
computed and assigned to the left hand side. This is exactly what is
done in assignment within `julia`, so in `julia` the above simply becomes:

```
julia> x = x - f(x)/fp(x)
```

Where `f(x)` is the function and `fp(x)` its derivative. This line
starts with a previously defined value of `x` and updates it
accordingly. 


The updating is continued -- by executing the exact same command --
until either the algorithm has gotten close enough our answer (it has
converged) or we have given up on it converging -- it took too long.

We can determine two ways that the number is close enough to the answer: 

* The sequence of `x`'s stop changing by much
* the values `f(x)` get close enough to zero.

Both concepts require a tolerance. For the first case, it is easiest
to stop when the printed value of `x` stops changing. This is 16
digits so may be problematic.

For the second we will use the square root of the floating point
representation limit given conveniently with:

```j
tol = sqrt(eps())
```


<i class="icon-eye-open"></i> <span class="label label-important">Approximate answers only</span>

It is important to realize that the actual answer is not likely to be
returned by Newton's method. If most cases, the answer will be
irrational and a floating point number ultimately is never better than
an approximation to an irrational number. This is why we need to be
clear by what we mean by "close enough".



Here is an example to find a zero of the cosine function:

```j
f(x) = cos(x)
fp(x) = -sin(x)
x = pi/2 + .3 ## some x0 initial guess
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
```

The resulting value of `x` we can see when evaluated is less the
tolerance in absolute value:

```j
f(x)
abs(f(x)) < tol
```

You can see in this case that the convergence happens quickly as soon as the algorithm gets close.

<hr />
`r I(page$new_problem("Try it yourself..."))`

... fill me in ...

`r I(page$new_problem("Try it yourself..."))`

... fill me in ...

<hr />


<h4>Using the function value to decide</h4>


Here we use the other concept of close to check. That is, we repeat
the expression until we find the answer to `abs(f(x)) < tol` to be
`true`. This idea generalizes to higher dimensions and is more in line
with the task at hand.



As with the _bisection method_, we define a function to do these comparisons:

```j
close_enough(x::Real, tol::Real) = abs(x) < tol
close_enough(x::Real) = close_enough(x, sqrt(eps()))
```

We revisit a problem from a previous project, finding zeroes of the
function $f(x) = \exp(x) - x^4$. We know from previous work that there
are three of them. Let's find one of them using a somewhat large
tolerance:


```j
f(x) = exp(x) - x^4
fp(x) = exp(x) - 4x^3
x = 2
x = x - f(x)/fp(x); (x, close_enough(f(x), 0.0001)) ## print out current x and close_enough
x = x - f(x)/fp(x); (x, close_enough(f(x), 0.0001))
x = x - f(x)/fp(x); (x, close_enough(f(x), 0.0001))
x = x - f(x)/fp(x); (x, close_enough(f(x), 0.0001))
```

Now that we are "close enough", let's see how close:

```j
f(x)
```


This last example suggests an algorithm for Newton's method: while the
value is not within tolerance repeat the algorithm _unless_ it has gone
on too long to have any hope of success.

This we can implement in `julia`, as it is similar to what we did for
the bisection method. The only new wrinkle is we keep track of a counter to
see if we have tried too many times, as defined below by `max_steps`.

```j
function nm(f, fp, x0)
   
    ## redefine to be self contained
    close_enough(x) = abs(x) < sqrt(eps())

    max_steps = 100
    ctr = 0

    update_guess(x) = x - f(x)/fp(x)

    x = update_guess(x0)

    while !close_enough(f(x)) && ctr < max_steps
        ctr += 1
        x = update_guess(x)
    end

    ## all done
    if ctr >= max_steps
        error("Method did not converge")
    else
        return (x, ctr)
    end
end
```

Here we try it:

```j
f(x) = x^3 - 5x + 1
fp(x) = 3x^2 - 5
nm(f, fp, 0)
```

<hr />


`r I(page$new_problem("Some question"))`

Let $f(x) = x^3 - 2x -5$. Apply Newton's method starting at $x=2$. What is your approximate root?


```{r echo=FALSE, results="asis"}
val = 2.094551481698199
cat(page$numeric_choice(val - .001, val + .001))
```

(Newton looked at this problem himself, though with a different method, getting the estimate $2.09455147$. http://www.math.uiuc.edu/documenta/vol-ismp/13_deuflhard-peter.pdf).

`r I(page$new_problem("Some question"))`

Repeat the problem of finding a root of $f(x) = \exp(x) - x^4$ starting
at $x=2$. How many steps does it take with the default tolerance used
in `nm`?



```{r echo=FALSE, results="asis"}
val = 4
cat(page$numeric_choice(val - .001, val + .001))
```

`r I(page$new_problem("Some question"))`

If we repeat with $f(x) = \exp(x) - x^4$ but start now at $x=8$ where
does the algorithm converge?


```{r echo=FALSE, results="asis"}
val = 8.61316945644141
cat(page$numeric_choice(val - .001, val + .001))
```



`r I(page$new_problem("Some question"))`

```{r echo=FALSE}
k <- floor(runif(1, 5, 15))
```

Let $f(x) = \sin(x) - \cos(`r k` x)$.
Starting at $\pi/(2 \cdot `r k`)$, solve for the root returned by Newton's method


```{r echo=FALSE, results="asis"}
f = function(x) sin(x) - cos(k*x)
fp = function(x) cos(x) + k*sin(k*x)
nm <- function(f, fp, x0) {
   tol = 10^(-8)
   x   = x0
   while(abs(f(x)) > tol) {
     x = x - f(x)/fp(x)
   }
   x
}
val <- nm(f, fp, pi/(2*k))
cat(page$numeric_choice(val - .001, val + .001))
```

<hr />

<span class="label label-warning">Example: Numeric derivatives</span>


In order to use Newton's method we need to evaluate by hand both $f(x)$ and
$f'(x)$. This isn't technically necessary, as we can numerically
approximate the latter, as we demonstrate next. Recall that we used in
a previous project a definition for an approximate first derivative
and defined a `'` operator. We load the work here with:

```j
using TeachersHelpers; load_gist("4530920")
```

This allows us to numerically differentiate $f(x)$ in the algorithm quite simply.

Let $f(x) = \exp(x) - x^4$ and set $x_0 = -1$. We can find a root with the following command:


```j
f(x) = exp(x) - x^4
nm(f, f', -1)
```

We could have done the same computation with the automatic derivative given by `D(f)`:

```j
nm(f, D(f), -1)
```



Note, had we supplied the derivative in this case we get the same
answer.

```j
nm(f, x -> exp(x) - 4x^3, -1)
```

This need not always be the case when using the approximate
derivative, as we stop not when we get to the answer, but when we are
within some tolerance. There is no guarantee we will have the same
sequence of points. We use the automatic derivative in the following
when we can, and otherwise the numeric derivative.




<i class="icon-eye-open"></i> <span class="label label-important">Not as efficient</span>

Not only are there more function evaluations, the convergence in
general is not as fast using numeric derivatives. For these examples,
this isn't a huge concern, but we wouldn't want to leave you with the
feeling this is the best thing to do.

<i class="icon-eye-open"></i> <span class="label label-important">The secant method</span>

Using an approximate derivative in Newton's method is similar to the
"secant method", which has a much longer history in root finding than
Newton's method (cf. http://en.wikipedia.org/wiki/Secant_method). The
secant method uses the secant line formed by the last two approximate
values instead of the tangent line at the last approximate value. Some
view it as the oldest numerical linear algebra tool, dating back 3000
years (Popakonstantinou,
http://scholarship.rice.edu/bitstream/handle/1911/20568/1442093.PDF?sequence=1).



`r I(page$new_problem("A polynomial case"))`

Let 

$$
f(x) = 4x^4 -5x^3 + 4x^2 -20x - 6
$$

Apply Newton's method with $x_0=0$ using a numeric derivative. What value does it converge to?



```{r echo=FALSE, results="asis"}
val <- -0.2779809487126034
cat(page$numeric_choice(val - .001, val + .001))
```



<span class="label label-warning">Example: a tougher derivative</span>

Let's try with a function where the derivative is not known easily. If we set

$$
f(x) = x^x - 2
$$

Can we find a root using Newton's method, where $x > 0$?

We graph the function to see, using a smallish interval at first:

```j
f(x) = x^x - 2
plot([f, x -> 0], 0, 2) | orender
```

Eyeing this, we pick an initial point, $1$, for Newton's method to the
right of the minimum, which appears to be around $x=.35$.

```j
nm(f, D(f), 1)
```



<span class="label label-warning">Example: Finding the average of some numbers the hard way</span>

Let's do a round about way of finding the average of 5 numbers. We
define our 5 values and a function which computes the squared
difference from a given value of these numbers.

```j
f(x) = sum([(x - i)^2 for i in vals])
```

(This can be more expressed more compactly with the "dot" notation:

```
f(x) = sum((x - vals).^2)
```

but this won't work with our approximate derivatives, so we'd need `f'` and `f''` below).


The minimum value of `f(x)` will occur at the mean of the 5
numbers. So instead of `f` we start with finding the (lone) critical
point of $f$ which is when $f'(x) = 0$. This means we pass the
derivative and its derivative (the second derivative of $f$) to
Newton's method.

```j
(x, ctr) = nm(D(f), D2(f), 0)
```

Here we see that indeed we found the mean (average) value:

```j
x - mean(vals)
```



`r I(nav$add("Various issues"))`




As great as Newton's method is, it won't always work for various
reasons, some of which are described in the following. Here is what
you need to keep in mind. Newton's method works well if

* $|f'(x)|$ is not too small (If the tangent line is nearly flat, the next guess is far from the previous)

* $|f^{\'\'}(x)|$ is not too big (function doesn't have so much curve that
  the tangent line is a poor approximation)

* The initial guess is not to far from a zero


#### The initial guess is no where near the end results

Let $f(x) = \sin(x) - x/4$ and $x_0 = 2\pi$. This value is deliberately a poor choice:

```j
f(x) = sin(x) - x/4
fp(x) = cos(x) - 1/4
nm(f, fp, 2pi)
```

Though `julia` makes this happens fast, note that the counter got to 20 before converging and the
answer is no where near the guess. This trace might show why

```{r echo=FALSE}
f <- function(x) sin(x) - x/4
fp <- function(x) cos(x) - 1/4
plot_nm(f, fp, 2*pi, -15, 20)
```

### Questions

 `r I(page$new_problem("Some question..."))`

* When $|f'(x)|$ is too close to $0$, the path can jump alot. In the figure, what was the longest jump?

  (x) from about 17 to -10 () from about -12 to -3 () from about 0 to -5

* The method did find a zero, but the initial guess was nowhere near the final zero. How close was the closest zero to the initial guess?
  
  () 8.75 () $2\pi$ (x) 3.8


#### Function has a poor shape

Let $f(x) = x^{1/3}$. We know the root is 0. Let's see what
happens if we use Newton's method. We have to be careful though as `julia` thinks that cube
roots of negative numbers are not a number `NaN`. (It wants to use a
not real, complex number for the answer.)

So we define our function using `julia`'s `cbrt` function as follows:

```j
f(x) = cbrt(x)
fp(x) = sign(x) * (1/3)*cbrt(x)/x
nm(f, fp, 2)
```

Despite our care with the derivative, the method did not converge in 200 steps. Can you see why from this trace?

```{r echo=FALSE}
cbrt = function(x) sign(x)*abs(x)^(1/3)
f = function(x) cbrt(x)
fp = function(x) (1/3)* 1/cbrt(x)^2

plot_nm(f, fp, 1, -16, 16, diff_tol=1, max_ctr=4)
```



### Questions
`r I(page$new_problem("Solve by hand"))`

For $f(x) = x^{1/3}$, simplify the expression

$$
x - f(x)/f'(x)
$$

What do you get?


```{r echo=FALSE, results="asis"}
choices <- c("XXX",
	     "-2x",
	     "XYZ"
	     )
ans <- 2
comment <- setNames(list("XXX", "Nope. You are just guessing?"), choices[-ans])

cat(page$radio_choice(choices, choices[ans],  inline=FALSE))
```



#### Cycles

Here is a pathological example where the value always cycles no matter
where you start, provided you avoid 0.

Let $f(x) = \sqrt(\mid x \mid)$. This is the one-sided square root function
turned into an even function. We could also have defined it by:

```j
f(x) = x >= 0 ? sqrt(x) : sqrt(-x)
```

where the ternary operator `a?b:c` looks at `a` and if true will use do `b` otherwise `c`.

This makes it easier to take the derivative of the function:

```j
fp(x) = x >=0 ? (1/2)*sqrt(x)/x : -(1/2)*sqrt(-x)/(-x)
```

To see what happens, lets start at $x=2$

```j
x = 2
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
```

Trying again with $x=3$ doesn't help:

```j
x = 3
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
```

No matter where you start (except at 0), this will cycle back and forth.




`r I(page$new_problem("print out graph and trace"))`




`r I(nav$add("Quadratic convergence"))`




How fast does Newton's method converge? It depends on the function of
course. For some functions, it may not converge at all. Suppose our
function $f(x)$ satisfies $f(\alpha) = 0$ and has a third derivative
near $\alpha$, which. Let $x_0, x_1, \dots$ be the sequence of points
generated by Newton's method and let $\Delta x_i = x_i - \alpha$ be
the error between the zero and the $i$th estimate.

Then the following holds:

$$
\Delta x_{i+1} = \frac{f\'\'(\alpha)}{f'(\alpha)}(\Delta x_i)^2 + \text{error}
$$

Where the error is basically a constant times $\Delta x_i^3$. So if
$\Delta x_i$ is small enough, this more or less says that the next
error is the square of the previous error times some bound involving
the second and first derivative. This nicely illustrates the three things that can go wrong:

* the start is not close to the zero. (Note, if $\Delta x_i > 1$ this grows!)
* The first derivative is close to $0$. (Then we are dividing by a small number and get a big bound.)
* The second derivative is not small. (Then the bound is also big.)


In practice, what this mean is that for good functions once `x` gets
close, successive `x` values should be differ at a decimal point that
is like the first, second, fourth, then eighth. This is termed
_quadratic convergence_.

Let's see. A quick graph of $f(x)$ below shows the answer to be near
0.4, where we begin

```j
f(x) = x^3 - 5x +2
fp(x) = 3x^2 - 5
x = .4
x = x - f(x)/fp(x)  ## differ at 1e-2
x = x - f(x)/fp(x)  ## differ at 1e-4
x = x - f(x)/fp(x)  ## differ at 1e-9
x = x - f(x)/fp(x)  ## differ at 1e-15
```

So essentially with a good guess, the convergence is very fast.

### Questions

`r I(page$new_problem("speed ..."))`
`r I(page$new_problem("speed ..."))`
`r I(page$new_problem("speed ..."))`





<!--- Finish this off -->
`r I(nav$write_footer())`
`r I(page$write_footer())`

